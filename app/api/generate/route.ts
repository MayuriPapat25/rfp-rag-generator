import { NextResponse } from "next/server";
import ollama from "ollama";
import { getDocuments, IDocumentEntry } from "@/lib/db"; // MongoDB document fetching
import { generateEmbedding } from "@/lib/embeddings"; // Embedding generation function
// Import FaissStore and OllamaEmbeddings from LangChain
import { FaissStore } from "@langchain/community/vectorstores/faiss";
import { ChatOllama, OllamaEmbeddings } from "@langchain/ollama";
import { Document } from "@langchain/core/documents"; // For LangChain Document type

// const OLLAMA_CHAT_MODEL = "llama2";
const OLLAMA_CHAT_MODEL = "qwen3:0.6b";
const OLLAMA_EMBEDDING_MODEL = "nomic-embed-text"; // This model is used for embeddings

// FormattedDocumentEntry interface remains the same, but we'll convert it to LangChain's Document type
interface FormattedDocumentEntry {
  id: string;
  filename: string;
  fileType: string;
  content: string;
  createdAt: number;
  projectName: string;
  embedding?: number[]; // Ensure embedding is here for similarity search
  type: "document_chunk"; // Explicit type for context construction
}

// NOTE: We no longer need this manual cosineSimilarity function
// as FaissStore will handle similarity calculations internally.
// function cosineSimilarity(vecA: number[], vecB: number[]): number { /* ... */ }

export async function POST(req: Request) {
  try {
    const { question, projectName } = await req.json();

    if (!question) {
      return NextResponse.json(
        { error: "Question is required" },
        { status: 400 }
      );
    }

    console.log(
      `API Generate: Received question: "${question}" for project: "${projectName}"`
    );

    // 1. Fetch all relevant documents for the specified project from MongoDB
    const allDocumentsFromDb = await getDocuments(projectName);
    console.log("allDocuments fetched from DB:", allDocumentsFromDb.length);

    // Prepare documents for FAISS: Convert to LangChain's Document format
    // and ensure they have content and embeddings.
    const documentsForFaiss: Document[] = allDocumentsFromDb
      .filter((doc) => doc.embedding && doc.embedding.length > 0 && doc.content) // Ensure valid embeddings and content
      .map(
        (doc) =>
          new Document({
            pageContent: doc.content,
            metadata: {
              id: doc._id.toString(), // Original MongoDB _id
              filename: doc.filename,
              fileType: doc.fileType,
              createdAt: doc.createdAt.getTime(),
              projectName: doc.projectName,
              // Store the pre-computed embedding in metadata for reconstruction
              // FaissStore.fromDocuments expects embeddings to be generated by embeddings model
              // If you have them pre-computed, you'll need to pass them or ensure FaissStore can use them
              // For now, we'll assume FaissStore.fromExistingIndex handles precomputed.
              // Or, for `fromDocuments`, it will generate new ones if not specified or if you prefer
            },
            // FaissStore.fromDocuments requires the embedding function to be passed,
            // and it will generate embeddings internally if not provided in `pageContent`.
            // If you already have embeddings, the most robust way is to save/load a pre-built index.
            // For dynamic in-memory index from DB, we create a temporary store with provided embeddings.
            // NOTE: LangChain's `fromDocuments` will re-embed if not explicitly handled.
            // A more direct way to use existing embeddings for an in-memory FAISS is shown below.
          })
      );

    // --- Create an in-memory FAISS index from the fetched documents ---
    // This assumes your `generateEmbedding` function (or `ollamaEmbeddings` instance) can
    // be used to pass to FaissStore for consistency, even if embeddings are pre-computed.
    // However, if you explicitly want to use the *exact* pre-computed embeddings:
    // FaissStore.fromDocuments takes `docs` and `embeddings` model. It will re-embed if needed.
    // To directly use *pre-computed* embeddings for an in-memory index, a common pattern
    // is to create a dummy FaissStore and then add documents with their embeddings.
    // Or, if your `FaissStore` implementation allows, pass a list of [embedding, Document] tuples.

    const embeddingsInstance = new OllamaEmbeddings({
      baseUrl: process.env.OLLAMA_API_BASE_URL || "http://localhost:11434", // Ensure this matches your Ollama setup
      model: OLLAMA_EMBEDDING_MODEL,
    });

    let vectorStore: FaissStore;

    if (documentsForFaiss.length > 0) {
      // This is a common pattern to create an in-memory FAISS index
      // by first creating a dummy one, then adding documents with their pre-computed embeddings.
      // This requires the documentsForFaiss to be compatible with FaissStore's `addVectors` method.
      // The simplest way with LangChain is to use `fromDocuments` which handles embedding.
      // If you absolutely MUST use pre-computed embeddings to build in-memory index:
      // You'd typically need to use `addVectors` which expects `vectors: number[][]` and `docs: Document[]`.
      // Let's assume for simplicity `fromDocuments` is suitable if we control embedding generation.
      // Otherwise, if `embedding` on Document is standard and FaissStore supports it:
      // FaissStore does not directly take pre-computed embeddings to build an index via `fromDocuments`.
      // It relies on the provided `embeddings` instance to generate them.

      // A better approach for pre-embedded data:
      // If the `embedding` field in your `FormattedDocumentEntry` contains the exact embeddings,
      // you would typically save and load a *persisted* FAISS index, or
      // manually construct the FAISS index if you are not using FaissStore.load/save.
      // For an in-memory FAISS index built from already embedded documents,
      // you would feed the content and then ensure the embeddings are used, or re-embed if acceptable.

      // Simplest valid path for in-memory FAISS with fetched docs:
      // Use `fromDocuments` which will re-embed content using `embeddingsInstance`.
      // This assumes that re-embedding here is acceptable or necessary for FaissStore.
      // If your MongoDB embeddings are truly authoritative and should be used directly,
      // then FaissStore.fromDocuments might not be the correct method here.
      // You would need a more direct FAISS library integration (e.g., `faiss-node` raw API)
      // or a custom builder for FaissStore to inject pre-computed embeddings.

      // Let's stick to using `fromDocuments` for simplicity, assuming `embeddingsInstance`
      // produces consistent embeddings with what's in MongoDB, or if you prefer re-embedding
      // at runtime to ensure consistency with the current embeddings model.
      vectorStore = await FaissStore.fromDocuments(
        documentsForFaiss,
        embeddingsInstance // FaissStore will use this to embed `pageContent`
      );
    } else {
      // Handle case where no documents are available
      return NextResponse.json(
        {
          error:
            "No relevant documents with embeddings found for this project.",
        },
        { status: 404 }
      );
    }

    console.log(
      `FAISS index created in-memory with ${documentsForFaiss.length} documents.`
    );

    // 2. Perform semantic search using FaissStore
    // The TOP_K is already set to 8
    const relevantDocs = await vectorStore.similaritySearch(question, TOP_K);
    console.log(
      `FAISS search found ${relevantDocs.length} relevant documents.`
    );

    // 3. Construct context text from the content of the most relevant items
    const contextText = relevantDocs
      .map((doc) => {
        // LangChain Document has pageContent and metadata
        return `Document Entry (Project: ${
          doc.metadata.projectName || "N/A"
        }):\nFilename: ${doc.metadata.filename}\nContent: ${doc.pageContent}`;
      })
      .join("\n\n---\n\n");

    // Log the final context sent to the LLM
    console.log(
      "API Generate: Final Context Sent to LLM (first 500 chars):\n",
      contextText.substring(0, 500) + (contextText.length > 500 ? "..." : "")
    );
    console.log("Complete contextText being sent to LLM:\n", contextText);

    // Prompt remains the same
    const prompt = `You are an expert in hosting platforms and information security, tasked with drafting official responses to critical RFP/RFQ questions. Your primary directive is to provide factual, direct, and concise answers.

    **CRITICAL GUIDELINES**:
    1.  Your sole source of truth is the "Provided Context" below.
    2.  If the exact information required to answer the RFP/RFQ question is NOT explicitly contained within the "Provided Context", you MUST respond with: "I do not have enough information in the knowledge base to answer this question."
    3.  DO NOT generate any information that is not directly supported by the "Provided Context".
    4.  Your final response MUST be ONLY the direct answer to the RFP/RFQ question. Do NOT include any introductory phrases, conversational elements, preambles, or explanations of your reasoning.

    Provided Context:
    ${
      contextText ||
      "No relevant information found in the provided context to answer the question."
    }

    ---

    RFP/RFQ Question: ${question}

    Direct Answer:`;

    console.log("Final prompt being sent to Ollama:\n", prompt);

    // Initiate a streaming chat session with the Ollama model.
    const ollamaResponseStream = await ollama.chat({
      model: OLLAMA_CHAT_MODEL,
      messages: [{ role: "user", content: prompt }],
      stream: true,
    });

    // Create a ReadableStream to pipe the Ollama response chunks directly to the client.
    const readableStream = new ReadableStream({
      async start(controller) {
        for await (const chunk of ollamaResponseStream) {
          controller.enqueue(chunk.message.content);
        }
        controller.close();
      },
    });
    console.log("readableStream created for response.");
    return new NextResponse(readableStream, {
      headers: {
        "Content-Type": "text/plain",
      },
    });
  } catch (error: any) {
    console.error("API Error /api/generate:", error);
    return NextResponse.json(
      { error: "Internal Server Error: " + error.message },
      { status: 500 }
    );
  }
}
